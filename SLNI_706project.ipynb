{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLNI 706 project- Sequence analysis course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Please make sure all import work\n",
    "\n",
    "#General imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from transformers import BertConfig\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#Project models imports\n",
    "import params\n",
    "import utils\n",
    "from bert_model import BertTransformer\n",
    "from training import train_snli\n",
    "import transformer_model\n",
    "import basic_rnn\n",
    "import rnn_combined_model\n",
    "import snli_dataset\n",
    "import main_slni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Data files- Please make sure you have them\n",
    "TRAIN_DATA_FILE= './data/snli_1.0/snli_1.0_train.jsonl'\n",
    "VAL_DATA_FILE='./data/snli_1.0/snli_1.0_dev.jsonl'\n",
    "TEST_DATA_FILE='./data/snli_1.0/snli_1.0_test.jsonl'\n",
    "\n",
    "#Needed directories -Please make sure you have them.\n",
    "MODELS_DIR='./saved_models/'\n",
    "RUNS_DIR='./experiments/'\n",
    "RESULTS_DIR='./results'\n",
    "\n",
    "#Please make sure you have this files under MODELS_DIR .\n",
    "#This is a file I have prepared containing the vocabulary , otherwise the dataset will take long to process.\n",
    "VOCAB_FILE='vocab_counter.pkl'\n",
    "TRAIN_TOKENIZED_FILE='train_tokenized_datapoints.pickle'\n",
    "VAL_TOKENIZED_FILE='val_tokenized_datapoints.pickle'\n",
    "\n",
    "#Best models-saved checkpoints\n",
    "BEST_MODEL_BASIC_RNN='./saved_models/best_models/best_basic_rnn.th'\n",
    "BEST_MODEL_TRANSFORMER='./saved_models/best_models/best_basic_rnn.th'\n",
    "BEST_MODEL_RNN_COMBINE='./saved_models/best_models/best_basic_rnn.th'\n",
    "BEST_MODEL_BERT='./saved_models/best_models/best_basic_rnn.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Set seed to reproduce results\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Set device\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first prepare the datasets for training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 549367 \n",
      "Validation dataset size: 9842 \n",
      "Count neutral: 182764\n",
      "Count contradiction: 183187\n",
      "Count entailment: 183416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = snli_dataset.SNLIDataset(data_path=TRAIN_DATA_FILE, saved_dir=MODELS_DIR, device=device,  vocab_file=VOCAB_FILE,tokenized_datapoints_file=TRAIN_TOKENIZED_FILE)\n",
    "print(f\"Train dataset size: {len(train_dataset)} \")\n",
    "\n",
    "train_vocab=train_dataset.vocab\n",
    "val_dataset = snli_dataset.SNLIDataset(data_path=VAL_DATA_FILE, saved_dir=MODELS_DIR, device=device, vocab_external=train_vocab,tokenized_datapoints_file=VAL_TOKENIZED_FILE)\n",
    "print(f\"Validation dataset size: {len(val_dataset)} \")\n",
    "\n",
    "#Count number of datapoint in each label\n",
    "count_neural=0\n",
    "count_entailment=0\n",
    "count_contradiction=0\n",
    "for datapoint in train_dataset.datapoints:\n",
    "    if datapoint[2]==\"neutral\":\n",
    "        count_neural+=1\n",
    "    elif datapoint[2]==\"contradiction\":\n",
    "        count_contradiction+=1\n",
    "    elif datapoint[2]==\"entailment\":\n",
    "        count_entailment+=1\n",
    "\n",
    "print(f\"Count neutral: {count_neural}\")\n",
    "print(f\"Count contradiction: {count_contradiction}\")\n",
    "print(f\"Count entailment: {count_entailment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset has a good balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data examples\n",
      "First sentence:A person on a horse jumps over a broken down airplane.\n",
      "Second sentence:A person is training his horse for a competition.\n",
      "Label:neutral\n",
      "\n",
      "First sentence:A person on a horse jumps over a broken down airplane.\n",
      "Second sentence:A person is at a diner, ordering an omelette.\n",
      "Label:contradiction\n"
     ]
    }
   ],
   "source": [
    "print(\"Data examples\")\n",
    "print(f\"First sentence:{train_dataset.datapoints[0][0]}\")\n",
    "print(f\"Second sentence:{train_dataset.datapoints[0][1]}\")\n",
    "print(f\"Label:{train_dataset.datapoints[0][2]}\\n\")\n",
    "\n",
    "print(f\"First sentence:{train_dataset.datapoints[1][0]}\")\n",
    "print(f\"Second sentence:{train_dataset.datapoints[1][1]}\")\n",
    "print(f\"Label:{train_dataset.datapoints[1][2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#utils.save_to_pickle(train_dataset.tokenized_datapoints, './saved_models/train_tokenized_datapoints.pickle')\n",
    "#utils.save_to_pickle(val_dataset.tokenized_datapoints, './saved_models/val_tokenized_datapoints.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Train Basic RNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_classes': 3, 'hidden_size': 512, 'embeddings_dim': 300, 'hidden_pre2_classifier_linear_dim': 256, 'hidden_pre1_classifier_linear_dim': 64, 'pad_token': '<pad>', 'dropout_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#This RNN configuration is constant\n",
    "print(params.RNN_CONFIG_CONSTANT_CONFIG)\n",
    "\n",
    "#We will tune the following hyperparams possibilities\n",
    "RNN_CONFIG_LIST=[{'run_name': 'lr:0.003_embedding:glove','num_epochs':1, 'lr':0.003, 'embedding_type': 'glove', 'checkpoint': None},\n",
    "                 {'run_name': 'lr:0.001_embedding:glove','num_epochs':1, 'lr':0.001, 'embedding_type': 'glove', 'checkpoint': None},\n",
    "                 {'run_name': 'lr:0.003_embedding:None','num_epochs':1, 'lr':0.003, 'embedding_type': None, 'checkpoint': None},\n",
    "                 {'run_name': 'lr:0.001_embedding:None','num_epochs':1, 'lr':0.001, 'embedding_type': None, 'checkpoint': None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training BasicRNN lr:0.003_embedding:glove \n",
      "Step 1000/17168 Train running loss: 0.68\n",
      "Step 2000/17168 Train running loss: 0.81\n",
      "Step 3000/17168 Train running loss: 0.88\n",
      "Step 4000/17168 Train running loss: 0.79\n",
      "Step 5000/17168 Train running loss: 0.95\n",
      "Step 6000/17168 Train running loss: 0.79\n",
      "Step 7000/17168 Train running loss: 0.79\n"
     ]
    }
   ],
   "source": [
    "BasicRNN_results_pd=main_slni.train_BasicRNN(train_dataset, val_dataset, RNN_CONFIG_LIST, device, params.RNN_CONFIG_CONSTANT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Show results\n",
    "utils.save_to_pickle(BasicRNN_results_pd, './saved_models/basic_rnn_results_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This RNN configuration is constant\n",
    "print(params.RNN_COMBINE_CONSTANT_CONFIG)\n",
    "\n",
    "#We will tune the following hyperparams possibilities\n",
    "RNN_COMBINE_CONFIG_LIST=[{'run_name': 'lr:0.0003_embedding:glove','num_epochs':1, 'lr':0.0003, 'embedding_type': 'glove', 'checkpoint': None},\n",
    "                         {'run_name': 'lr:0.0001_embedding:glove','num_epochs':1, 'lr':0.0001, 'embedding_type': 'glove', 'checkpoint': None},\n",
    "                         {'run_name': 'lr:0.0003_embedding:None','num_epochs':1, 'lr':0.0003, 'embedding_type': None, 'checkpoint': None},\n",
    "                         {'run_name': 'lr:0.0001_embedding:None','num_epochs':1, 'lr':0.0001, 'embedding_type': None, 'checkpoint': None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RNNCombine_results_pd=main_slni.train_RNNCombine(train_dataset, val_dataset, RNN_COMBINE_CONFIG_LIST, device, params.RNN_COMBINE_CONSTANT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "utils.save_to_pickle(RNNCombine_results_pd, './saved_models/rnn_combine_results_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This RNN configuration is constant\n",
    "print(params.TRANSFORMER_CONSTANT_CONFIG)\n",
    "\n",
    "TRANSFORMER_CONFIG_LIST=[{'run_name': 'lr:5e-5_embedding:glove', 'num_epochs':1, 'lr':5e-5, 'embedding_type':'glove', 'checkpoint':None},\n",
    "                         {'run_name': 'lr:3e-5_embedding:glove', 'num_epochs':1, 'lr':3e-5, 'embedding_type':'glove', 'checkpoint':None},\n",
    "                         {'run_name': 'lr:5e-5_embedding:glove', 'num_epochs':1, 'lr':5e-5, 'embedding_type':None, 'checkpoint':None},\n",
    "                         {'run_name': 'lr:3e-5_embedding:glove', 'num_epochs':1, 'lr':3e-5, 'embedding_type':None, 'checkpoint':None}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Transformer_results_pd=main_slni.train_Transformer(train_dataset, val_dataset, TRANSFORMER_CONFIG_LIST, device,params.TRANSFORMER_CONSTANT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "utils.save_to_pickle(Transformer_results_pd, './saved_models/transformer_results_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(params.BERT_CONSTANT_CONFIG)\n",
    "\n",
    "\n",
    "BERT_CONFIG_LIST=[{'run_name': ' lr:5e-5', 'num_epochs':1, 'lr':5e-5,'embedding_type': 'bert', 'checkpoint': None},\n",
    "                  {'run_name': ' lr:5e-5', 'num_epochs':1, 'lr':3e-5,'embedding_type': 'bert', 'checkpoint': None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BERT_results_pd=main_slni.train_Bert(train_dataset, val_dataset, BERT_CONFIG_LIST, device,**params.BERT_CONSTANT_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "utils.save_to_pickle(BERT_results_pd, './saved_models/bert_results_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#SHOW RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run best results\n",
    "#combine and show comapriosm graphs\n",
    "\n",
    "#Attentions graphs\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
